BACKPROPOGATION


from random import seed
import pandas as pd
import numpy as np


def split_dataset(dataset, train_perc=0.8,test_perc = 0.2):
    np.random.shuffle(dataset)
    data_len = len(dataset)
    train_index = int(data_len*train_perc) # last index from the dataset array that will go into training
    
    train = dataset[:train_index,:]  
    test = dataset[train_index:,:] 
    return (train, test)

def sigmoid(activation):
    return 1.0 / (1.0 + np.exp(-activation))
    
def compute_loss(prediction, actual):
    #return -sum(actual*log(prediction))
    return 0.5*np.sum((actual.T-prediction)*(actual.T-prediction))

def back_prop(train_X,W1,W2,layer1_output,layer2_output,actual_output):
    #find error in output unit
    difference = actual_output.T - layer2_output    
    delta_output = layer2_output*(1-layer2_output)*difference
    delta_hidden = layer1_output*(1-layer1_output)*W2.T.dot(delta_output)
    deltaW2 = lr*(delta_output.dot(layer1_output.T)/n_train) 
    deltaW1 = lr*(delta_hidden.dot(train_X)/n_train) 
    
    return (deltaW1,deltaW2)
    
def train_network(train_X, train_y):
    n_input = train_X.shape[1]  # the number of columns in the training data
    W1=np.random.random((n_hidden,n_input))
    W2=np.random.random((num_classes,n_hidden ))
    for epoch in range(n_epoch):
        layer1_output = sigmoid(W1.dot(train_X.T))
        layer2_output = sigmoid(W2.dot(layer1_output))
        
        (deltaW1,deltaW2)= back_prop(train_X,W1,W2,layer1_output,layer2_output,train_y)
        print(deltaW1[:5])
        W2 = W2+deltaW2
        W1 = W1+deltaW1
        if epoch%1000 == 0:
            loss = compute_loss(layer2_output,train_y)
            print(str.format('Loss in {0}th epoch is {1}',epoch,loss))
        
            
    return (W1,W2)

def evaluate(test_X,test_y,params):
    (W1,W2) = params
    layer1_output = sigmoid(W1.dot(test_X.T))
    final = sigmoid(W2.dot(layer1_output))
    
    prediction = final.argmax(axis=0)    
    return np.sum(prediction==test_y)/len(test_y)    

def convert_to_OH(data,num_classes):
    #create an array to store the one hot vectors
    one_hot = np.zeros((len(data),num_classes))
    one_hot[np.arange(len(data)),data] = 1
    return one_hot


np.random.seed(0)
# load and prepare data
#filename = 'seeds_dataset.csv'
filename = 'D:/Machine_Learning Programs/diabetes.csv'
df = pd.read_csv(filename,dtype=np.float64)
dataset = np.array(df)

#normalize data
min_data = dataset.min(axis = 0)
max_data = dataset.max(axis = 0)

#normalize all fields except the last column(class)
dataset[:,0:-1] = (dataset[:,0:-1] - min_data[0:-1])/(max_data[0:-1] - min_data[0:-1])
(train, test) = split_dataset(dataset)
print(train[:5,:-1])
#train = dataset 
n_train = len(train)
n_test = len(test)

# evaluate algorithm
lr = 0.8
n_epoch =15000

#determine the number of classes
num_classes = len(np.unique(dataset[:,-1]))
train_one_hot = convert_to_OH(train[:,-1].astype(int),num_classes)

n_hidden = 15

params = train_network(train[:,:-1],train_one_hot) 
accuracy = evaluate(test[:,:-1],test[:,-1],params)*100
print('Mean Accuracy: %.3f%%' % accuracy)

--------------------------------------------------------------------------------------------------------------------------

BAYES_NET


import numpy as np
import pandas as pd
import csv

from pgmpy.estimators import MaximumLikelihoodEstimator 
from pgmpy.models import BayesianModel

from pgmpy.inference import VariableElimination #Read the attributes

raw_data = pd.read_csv('D:/Machine_Learning Programs/data-train-5.csv')
print("Input Data and Shape")
print(raw_data.shape)
data = pd.DataFrame(raw_data,columns=['A', 'G', 'CP','BP','CH','ECG','HR','EIA','HD'])
#select the train data
data_train = data[: int(data.shape[0] * 0.80)]
#print(data_train)
# Display the data

#print('Few examples from the dataset are given below')
#print(heartDisease.head())
#print('\nAttributes and datatypes')
#print(heartDisease.dtypes)

# Model Baysian Network
# provide names for the categorical variables
variable_map = { \
"A": ["< 45", "45--55", "\geq 55"], \
"G": ["Female", "Male"], \
"CP": ["Typical", "Atypical", "Non-Anginal", "None"], \
"BP": ["Low", "High"], \
"CH": ["Low", "High"], \
"ECG": ["Normal", "Abnormal"], \
"HR": ["Low", "High"], \
"EIA": ["No", "Yes"], \
"HD": ["No", "Yes"] \
}
#creating the baysian network
model = BayesianModel([('HD', 'CP'),('G', 'BP'),('A', 'CH'),('G', 'CH'),('HD', 'ECG'),('HD',
'HR'),('BP', 'HR'),('A', 'HR'),('BP','HD'),('CH', 'HD'),('HD','EIA')])

#model = BayesianModel([('age', 'trestbps'), ('age', 'fbs'), ('sex', 'trestbps'), ('sex', 'trestbps'), ('exang', 'trestbps'),('trestbps','heartdisease'),('fbs','heartdisease'), ('heartdisease','restecg'),('heartdisease','thalach'),('heartdisease','chol')])

# Learning CPDs using Maximum Likelihood Estimators 
print('\nLearning CPDs using Maximum Likelihood Estimators...'); 
model.fit(data, estimator=MaximumLikelihoodEstimator)

# Inferencing with Bayesian Network
print('\nInferencing with Bayesian Network:')
HeartDisease_infer = VariableElimination(model)
for cpd in model.get_cpds():
 print("CPD of {variable}:".format(variable=cpd.variable))
 print(cpd)
 print(model.check_model())
#Computing the probability of HD given as a Vairiable. 
print('\n1.Probability of HeartDisease given Gender=Female')

q= HeartDisease_infer.query(variables=['HD'],evidence={'G': 1}) 
print(q['HD'])

print('\n2. Probability of HeartDisease given BP= Low')

q= HeartDisease_infer.query(variables=['HD'], evidence={'BP': 1})
print(q['HD'])


--------------------------------------------------------------------------------------------------------------------------


EM_KMeans


import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.mixture import GMM
from sklearn.cluster import KMeans
# Importing the dataset
data = pd.read_csv("D:/Machine_Learning Programs/xclara.csv")
print("Input Data and Shape")
print(data.shape)
data.head()
# Getting the values and plotting it
f1 = data['V1'].values
f2 = data['V2'].values
X = np.array(list(zip(f1, f2)))
plt.scatter(f1, f2, c='black',s=7)
plt.show()

kmeans = KMeans(3, random_state=0)
labels = kmeans.fit(X).predict(X)
centroids = kmeans.cluster_centers_
plt.scatter(X[:, 0], X[:, 1], c=labels, s=20, cmap='viridis');
plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='#050505')
plt.show()

gmm = GMM(n_components=3).fit(X)
labels = gmm.predict(X)
# for ploting
probs = gmm.predict_proba(X)
size = 10 * probs.max(1) ** 3
#print(probs[:300].round(4))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=size, cmap='viridis');
plt.show()


------------------------------------------------------------------------------------------------------------------


LOCALLY WEIGHTED REGRESSION



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
tou= 0.5
X_train = np.array(list(range(3, 33)) )
X_train=X_train[:,np.newaxis]
y_train = np.array([1,2,1,2,1,1,3,4,5,4,5,6,5,6,7,8,9,10,11,11,12,11,11,10,12,11,11,10,9,8]) #,2,13
X_test = np.array([i/10. for i in range(400)])
X_test=X_test[:,np.newaxis]
y_test = []
# find euclidean distance between train and test points
for r in range(len(X_test)):
    
        wts=np.exp(-np.sum((X_train-X_test[r])**2,axis=1)/(2*tou)**2)
        W=np.diag(wts)
        factor1 = np.linalg.inv(X_train.T.dot(W).dot(X_train)) # find inverse of (X.T*W*X)
        parameters=factor1.dot(X_train.T).dot(W).dot(y_train)    # final values of theta
        prediction=X_test[r].dot(parameters)
        y_test.append(prediction)
   
y_test = np.array(y_test)
plt.plot(X_train.squeeze(), y_train, 'o') 
plt.plot(X_test.squeeze(), y_test, '*')
plt.show()


------------------------------------------------------------------------------------------------------------------


NAIVE_BAYES



import pandas as pd

def preprocess(df):

    count={}

    numuniquevals={}

    class_count={}

    for col in df.columns[:-1]:

        count[col]={}

        numuniquevals[col]=len(df.loc[:,col].unique())

        

    for r in range(len(df)):

        cls=df.iloc[r,-1]

        if cls not in class_count:

            class_count[cls]=0

        class_count[cls]+=1

        for col in df.columns[:-1]:

            val=df.loc[r,col]

            if cls not in count[col]:

                count[col][cls]={}

            if val not in count[col][cls]:

                count[col][cls][val]=0

              

            count[col][cls][val]+=1

    

    return (count,numuniquevals,class_count)

    



df=pd.read_csv('charss.csv')

test=pd.read_csv('testnaivebayes.csv')



print(df)



(count,numuniquevals,class_count)=preprocess(df)



classes=list(df.iloc[:,-1].unique())

acc=0

for r in range(len(test)):

    pl=[]

    for cls in classes:

        p=1

        for col in df.columns[:-1]:

            val=test.loc[r,col]

            if val in count[col][cls]:

                p*=(count[col][cls][val]+1)/(class_count[cls]+numuniquevals[col])

            else:

                p*=1/(class_count[cls]+numuniquevals[col])

            

        p*=(class_count[cls]/len(df))

        pl.append(p)

    predindx=pl.index(max(pl))

    print(list(test.loc[r,:]))

    print('The prediction for record is',classes[predindx]) 

    x=[]

    x=test.iloc[:,-1]

  

    if(x[r]==classes[predindx]):

        acc=acc+1

print("accuracy is =",acc,"  ",acc/len(test))